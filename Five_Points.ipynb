{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle, argparse, os, sys\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7efb4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_drive_data(data_path = None):\n",
    "    '''\n",
    "        read csv data and remove rows with sentiment labelled as not relevant\n",
    "        convert sentiment labels from str to int\n",
    "        \n",
    "        parms: data path str\n",
    "        \n",
    "        return: pandas dataframe\n",
    "    '''\n",
    "    #vader sentiment scorer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    data = pd.read_csv(data_path)\n",
    "    data = data[[\"sentiment\", \"text\"]]\n",
    "    data = data[data[\"sentiment\"] != 'not_relevant']\n",
    "    data[\"sentiment\"] = data[\"sentiment\"].apply(lambda x: int(x))\n",
    "    data[\"sentiment\"] = data[\"sentiment\"].apply(lambda x: x-1)\n",
    "    data[\"vader_sentiment\"] = data[\"text\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b993b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    '''\n",
    "        Split data into two parts\n",
    "        \n",
    "        params: pandas dataframe\n",
    "        return: two pandas dataframe and save as pickle files\n",
    "    '''\n",
    "    # split data into training_validation(0.9), and testing sets(0.1)\n",
    "    train, test = train_test_split(data, test_size=0.1, random_state=42, shuffle=True, \\\n",
    "                                         stratify=data[\"sentiment\"])\n",
    "\n",
    "    train_pkl = open(\"drive_pkl/train_drive.pkl\", \"wb\")\n",
    "    pickle.dump(train, train_pkl)\n",
    "    train_pkl.close()\n",
    "        \n",
    "    test_pkl = open(\"drive_pkl/test_drive.pkl\", \"wb\")\n",
    "    pickle.dump(test, test_pkl)\n",
    "    test_pkl.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03019c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAE_macro(prediction, ground_truth):\n",
    "    '''\n",
    "        calculate the macroaver_aged mean absolute error for \n",
    "        the predicted labels and the true labels\n",
    "\n",
    "        params: arrays\n",
    "        return: int\n",
    "    '''\n",
    "    # create the prediction and ground truth pairs for\n",
    "    # each class \n",
    "    label_dict = {\n",
    "            \"label0\": [],\n",
    "            \"label1\": [],\n",
    "            \"label2\": [],\n",
    "            \"label3\": [],\n",
    "            \"label4\": []\n",
    "        }\n",
    "    \n",
    "    for pred, golden in zip(prediction, ground_truth):\n",
    "        if golden == 0:\n",
    "            label_dict[\"label0\"].append((pred, golden))\n",
    "        elif golden == 1:\n",
    "            label_dict[\"label1\"].append((pred, golden))\n",
    "        elif golden == 2:\n",
    "            label_dict[\"label2\"].append((pred, golden))\n",
    "        elif golden == 3:\n",
    "            label_dict[\"label3\"].append((pred, golden))\n",
    "        elif golden == 4:\n",
    "            label_dict[\"label4\"].append((pred, golden))\n",
    "    \n",
    "    class_avg = []\n",
    "    # Calculate MAE macro\n",
    "    for label in range(0, 5):\n",
    "        doc_len = len(label_dict[\"label%s\" % label])\n",
    "        total_distance = 0\n",
    "        for doc in label_dict[\"label%s\" % label]:\n",
    "            total_distance += abs(doc[0] - doc[1])\n",
    "        \n",
    "        average_distance = total_distance / doc_len\n",
    "        class_avg.append(average_distance)\n",
    "        \n",
    "    mae = sum(class_avg) / 5\n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18cc0681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_earth_mover(prediction, ground_truth):\n",
    "    \n",
    "    # create the prediction and ground truth pairs for\n",
    "    # each class \n",
    "    pred_dict = Counter(prediction)\n",
    "    label_dict = Counter(ground_truth)\n",
    "    \n",
    "    \n",
    "    \n",
    "    emd = 0\n",
    "    for j in range(0,4):\n",
    "        label_distance = 0\n",
    "        for i in range(0,j+1):\n",
    "            label_distance += (pred_dict[i]/len(prediction) - label_dict[i]/len(ground_truth))\n",
    "        emd += abs(label_distance)\n",
    "    return emd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f503d97",
   "metadata": {},
   "source": [
    "### Extract TF-IDF features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d17990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(data):\n",
    "    '''\n",
    "        remove stopwords and unfrequent words and convert words into features\n",
    "        \n",
    "        params: data frame\n",
    "        return: tfidf matrix\n",
    "    '''\n",
    "    # exclude every punctuation\n",
    "    tfidf=TfidfVectorizer(dtype=np.float32, max_df=0.80, min_df=2,stop_words='english', use_idf=True, \\\n",
    "                          smooth_idf=True, sublinear_tf=True)\n",
    "    # exclude every punctuation except exclaimation mark\n",
    "#     tfidf=TfidfVectorizer(dtype=np.float32, max_df=0.80, min_df=2,stop_words='english', use_idf=True, \\\n",
    "#                       smooth_idf=True, sublinear_tf=True, token_pattern =r\"[^ ().,@#$%^&*]+\")\n",
    "    tfidf_matrix=tfidf.fit_transform(data['text'])\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db403a6",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811db15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle, argparse, os, sys\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "\n",
    "import torch.functional as F\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fcf134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 16\n",
    "NUM_LAYERS = 1\n",
    "MAX_SEQ_LENGTH = 20\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCH = 3\n",
    "BATCH_SIZE = 10\n",
    "WEIGHT_DECAY = 0\n",
    "GLOVE_PATH = \"glove.6B 2\"\n",
    "DROPOUT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aefd95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    '''\n",
    "        create token list for the tweets and create a vocabulary dictionary \n",
    "        to replace unfrequent words with UNKA\n",
    "        \n",
    "        Also create a token_to_idx dictionary include special token <PAD>\n",
    "        \n",
    "        Besides, convert sentiment label into integers and create one hot vectors\n",
    "\n",
    "        param: pandas dataframe\n",
    "        return: token_to_idx, dict\n",
    "                #one hot vector embedding of labels, tensor\n",
    "    '''\n",
    "    \n",
    "    # remove stopwords\n",
    "    # tokenize tweets\n",
    "    # create vocab list for checking frequency\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text_list = []\n",
    "    vocab_list = []\n",
    "    for tweet in tqdm(data[\"text\"]):\n",
    "        tweet = re.sub(\"\\@[A-Za-z]+\", \"\", tweet)\n",
    "        tweet = re.sub(\"http\\S+\", \"\", tweet)\n",
    "        tweet = re.sub(\"[.!,@#$%^&*]\", \"\", tweet)\n",
    "        tokens = word_tokenize(tweet)\n",
    "        filtered_text = [w for w in tokens if not w in stop_words] \n",
    "        filtered_text_list.append(filtered_text)\n",
    "        vocab_list += filtered_text\n",
    "    data[\"filtered_text\"] = filtered_text_list\n",
    "    \n",
    "    # replace unfrequent words with \"UNKA\"\n",
    "    # create token_to_idx dict\n",
    "    vocab_dict = Counter(vocab_list)\n",
    "    token_to_replace = {k: v for k, v in vocab_dict.items() if v < 3}\n",
    "    token_to_idx = {\"<PAD>\":0}\n",
    "    token_list = []\n",
    "    \n",
    "    for tweet in tqdm(data[\"filtered_text\"]):\n",
    "        for token_idx in range(len(tweet)):\n",
    "            if tweet[token_idx] in token_to_replace:\n",
    "                tweet[token_idx] = \"UNKA\"\n",
    "        token_list += tweet\n",
    "    for token in Counter(token_list).keys():\n",
    "        if token not in token_to_idx:\n",
    "            token_to_idx[token] = len(token_to_idx)\n",
    "            \n",
    "#     # create index for sentiment labels\n",
    "#     sentiment_to_idx = {\"neutral\":0, \"negative\":1, \"positive\":2}\n",
    "# #     data[\"label_idx\"] = data[\"airline_sentiment\"].apply(lambda x: sentiment_to_idx[x])\n",
    "#     data[\"label_idx\"] = data[\"airline_sentiment\"].apply(lambda x: sentiment_to_idx[x])\n",
    "#     data[\"sentiment\"] = data[\"sentiment\"].apply(lambda x: x-1)\n",
    "#     labels_lists = torch.tensor(data[\"label_idx\"])\n",
    "#     one_hot_vector_emb = torch.zeros((labels_lists.size(dim=0), labels_lists.max()+1))\n",
    "#     one_hot_vector_emb[torch.arange(labels_lists.size(dim=0)),labels_lists] = 1\n",
    "    \n",
    "    return token_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "632d42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecnode_sequence(tweet, token_to_idx):\n",
    "    '''\n",
    "        convert tweet into sequence of index using token_to_idx dict\n",
    "\n",
    "        params: str and token to index dictionary\n",
    "        return: eccoded sequence list in tensor\n",
    "    '''\n",
    "    encoded_seq_list = []\n",
    "    \n",
    "    for token in tweet:\n",
    "        try:\n",
    "            encoded_seq_list.append(token_to_idx[token])\n",
    "        except KeyError:\n",
    "            encoded_seq_list.append(token_to_idx[\"UNKA\"])\n",
    "            \n",
    "    return torch.tensor(encoded_seq_list, dtype=torch.long)\n",
    "\n",
    "\n",
    "# def pad_sequence(encoded_sequence, max_seq_length, vader_sentiment):\n",
    "#     # with vader sentiment feature\n",
    "#     '''\n",
    "#         truncate or pad the sequence with 0 if the sequence is shorter \n",
    "#         than the number defined for training:max_seq_length\n",
    "        \n",
    "#         parmas: tensor of encoded sequence\n",
    "#         return: tensor of padded sequence\n",
    "        \n",
    "#     '''\n",
    "#     padded_sentence = torch.zeros(max_seq_length, dtype = torch.long)\n",
    "#     value_to_pad = min(len(encoded_sequence), max_seq_length)\n",
    "#     padded_sentence[:value_to_pad] = encoded_sequence[:value_to_pad]\n",
    "#     padded_sentence[-1] = vader_sentiment\n",
    "#     return padded_sentence\n",
    "\n",
    "def pad_sequence(encoded_sequence, max_seq_length):\n",
    "    # without vader sentiment feature\n",
    "    '''\n",
    "        truncate or pad the sequence with 0 if the sequence is shorter \n",
    "        than the number defined for training:max_seq_length\n",
    "        \n",
    "        parmas: tensor of encoded sequence\n",
    "        return: tensor of padded sequence\n",
    "        \n",
    "    '''\n",
    "    padded_sentence = torch.zeros(max_seq_length, dtype = torch.long)\n",
    "    value_to_pad = min(len(encoded_sequence), max_seq_length)\n",
    "    padded_sentence[:value_to_pad] = encoded_sequence[:value_to_pad]\n",
    "    return padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a703eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_path):\n",
    "    '''\n",
    "        open the glove pre-trained embeddings and process it\n",
    "        save the word embedding vectors to pytorch tensor and \n",
    "        the words and word_to_idx dictionary to pickle files\n",
    "\n",
    "        params: glove path\n",
    "        return: word embedding tensor\n",
    "    '''\n",
    "    words = []\n",
    "    idx = 0\n",
    "    word2idx = {}\n",
    "    vectors = []\n",
    "\n",
    "    stop = 0\n",
    "    with open(f'{glove_path}/glove.6B.50d.txt', 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "            vect = torch.tensor(vect, dtype=torch.double)\n",
    "            vect = torch.reshape(vect, [1, 50])\n",
    "            vectors.append(vect)\n",
    "\n",
    "    vectors = torch.cat(vectors)\n",
    "    pickle.dump(words, open(f'{glove_path}/6B.50_words.pkl', 'wb'))\n",
    "    pickle.dump(word2idx, open(f'{glove_path}/6B.50_idx.pkl', 'wb'))\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a833d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_glove(vectors, glove_path, token_to_idx):\n",
    "    '''\n",
    "        open the words and word_to_index pickle files\n",
    "        and map every token in the training data with a vector\n",
    "\n",
    "    '''\n",
    "    # load glove embeddings\n",
    "    words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))\n",
    "    word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))\n",
    "    glove = {w: vectors[word2idx[w]] for w in words}\n",
    "\n",
    "    # load training data\n",
    "    embedding_weights = np.zeros((len(token_to_idx), EMBEDDING_DIM))\n",
    "\n",
    "    for token, index in token_to_idx.items():\n",
    "        if token == \"UNKA\":\n",
    "            embedding_weights[index, :] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "        elif index == 0: #padding\n",
    "            embedding_weights[index, :] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "        else:\n",
    "            try:\n",
    "                embedding_weights[index, :] = glove[token]\n",
    "            except KeyError:\n",
    "                embedding_weights[index, :] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "\n",
    "    return torch.tensor(embedding_weights, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e70d6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    def __init__(self, token_to_idx, sentiment_to_idx):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        self.embedding_dim = EMBEDDING_DIM\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.vocab_size = len(token_to_idx)\n",
    "        self.tagset_size = len(sentiment_to_idx)\n",
    "        self.bidirectional = False\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size, \n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx = 0\n",
    "            )\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim, \n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional=self.bidirectional)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        if self.bidirectional:\n",
    "            self.hidden2tag = nn.Linear(2 * self.hidden_dim, self.tagset_size)\n",
    "        else:\n",
    "            self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\n",
    "            \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sentence, prev_state):\n",
    "        sentence = sentence.long()\n",
    "        embeds = self.word_embedding(sentence)\n",
    "        lstm_out, state = self.lstm(embeds, prev_state)\n",
    "\n",
    "        # add a fully connected layer to convert the high level info into our goal\n",
    "        # only use the last output of the lstm layer for many to one\n",
    "        lstm_out = self.hidden2tag(lstm_out[:, -1, :])\n",
    "        return lstm_out, state\n",
    "\n",
    "    def load_embedding(self, embedding_weights):\n",
    "        # using pre-trained embedding\n",
    "        self.word_embedding.load_state_dict({'weight': embedding_weights})\n",
    "\n",
    "    def init_state(self):\n",
    "        if self.bidirectional:\n",
    "            return (torch.zeros(2 * self.num_layers, BATCH_SIZE, self.hidden_dim),\n",
    "                    torch.zeros(2 * self.num_layers, BATCH_SIZE, self.hidden_dim))\n",
    "        else:\n",
    "            return (torch.zeros(self.num_layers, BATCH_SIZE, self.hidden_dim),\n",
    "                    torch.zeros(self.num_layers, BATCH_SIZE, self.hidden_dim))\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seq_list, label_list, token_to_idx, vader_sentiment):\n",
    "        self.seq_list = seq_list\n",
    "        self.label_list = label_list\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.vader_sentiment = vader_sentiment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_list)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # Select sample\n",
    "#         # with vader sentiment feature\n",
    "#         seq = self.seq_list[index]\n",
    "#         label_list = self.label_list[index]\n",
    "#         vader_score = self.vader_sentiment[index]\n",
    "\n",
    "#         sentence_input = ecnode_sequence(seq, self.token_to_idx)\n",
    "#         padded_sentence = pad_sequence(sentence_input, MAX_SEQ_LENGTH, vader_score)\n",
    "#         return padded_sentence, label_list\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        # without vader sentiment feature\n",
    "        seq = self.seq_list[index]\n",
    "        label_list = self.label_list[index]\n",
    "        vader_score = self.vader_sentiment[index]\n",
    "\n",
    "        sentence_input = ecnode_sequence(seq, self.token_to_idx)\n",
    "        padded_sentence = pad_sequence(sentence_input, MAX_SEQ_LENGTH)\n",
    "\n",
    "\n",
    "        return padded_sentence, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73516b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_file):\n",
    "    # load training_file\n",
    "    train_file = open(training_file, \"rb\")\n",
    "    data = pickle.load(train_file)\n",
    "    train_file.close()\n",
    "    \n",
    "    # prepare data, remove stopwords, replace unfrequent words and create a token\n",
    "    # to index dictionary\n",
    "    token_to_idx = prepare_data(data)\n",
    "    \n",
    "    X = data[\"filtered_text\"].tolist()\n",
    "    Y = data[\"sentiment\"].tolist()\n",
    "    vader_sentiment = data[\"vader_sentiment\"].tolist()\n",
    "    \n",
    "    # create a tokenizer pickle file for cache, only need to run when first executing\n",
    "    tokenizer = {'token_to_idx': token_to_idx}\n",
    "    tokenizer_file = open(\"drive_pkl/tokenizer.pkl\", \"wb\")\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n",
    "    tokenizer_file.close()\n",
    "    \n",
    "    # create glove vectors\n",
    "    vectors = load_glove(GLOVE_PATH)\n",
    "    # create word embedding weight matrix for training\n",
    "    embedding_weights = word_to_glove(vectors, GLOVE_PATH, token_to_idx)\n",
    "    torch.save(embedding_weights, 'drive_pkl/embedding_weights.pt')\n",
    "        \n",
    "    # load the saved files, only need to run when already saved the files\n",
    "    tokenizer_file = open(\"drive_pkl/tokenizer.pkl\", \"rb\")\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "    tokenizer_file.close()\n",
    "    token_to_idx = tokenizer['token_to_idx']\n",
    "    embedding_weights = torch.load('drive_pkl/embedding_weights.pt')\n",
    "\n",
    "    # initialize the RNN model\n",
    "    sentiment_to_idx = {\"Highly Negative\":1, \"Negative\":2, \"Neutral\":3, \"Positive\":4, \"Highly Positive\":5}\n",
    "    model = RNNTagger(token_to_idx, sentiment_to_idx) \n",
    "    model.load_embedding(embedding_weights)\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # dataset generator\n",
    "    training_set = Dataset(X, Y, token_to_idx, vader_sentiment)\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print('Start training...')\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print('Epoch %d:' % (epoch+1))\n",
    "        state_h, state_c = model.init_state()\n",
    "        \n",
    "        prediction = None\n",
    "        ground_truth = None\n",
    "        losses = []\n",
    "        for sequence, targets in tqdm(training_generator):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            \n",
    "            if sequence.shape[0] != 10:\n",
    "                 state_h, state_c = (torch.zeros(NUM_LAYERS, sequence.shape[0], HIDDEN_DIM),\n",
    "                    torch.zeros(NUM_LAYERS, sequence.shape[0], HIDDEN_DIM))\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "             \n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_space, (state_h, state_c) = model(sequence, (state_h, state_c))\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            targets = targets.long()\n",
    "            loss = loss_function(tag_space, targets)\n",
    "\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            # Store history to calculate accuracy\n",
    "            for i in range(sequence.shape[0]):\n",
    "                if prediction is None:\n",
    "                    prediction = torch.argmax(tag_space[i,:]).cpu().numpy()\n",
    "                else:\n",
    "                    prediction = np.append(prediction, torch.argmax(tag_space[i,:]).cpu().numpy())\n",
    "                if ground_truth is None:\n",
    "                    ground_truth = targets[i].cpu().numpy()\n",
    "                else:\n",
    "                    ground_truth = np.append(ground_truth, targets[i].cpu().numpy())\n",
    "                                \n",
    "        losses = torch.tensor(losses)\n",
    "        train_perplexity = math.exp(torch.mean(losses))\n",
    "        print(prediction)\n",
    "        print(ground_truth)\n",
    "        mae = calculate_MAE_macro(prediction, ground_truth)\n",
    "        emd = calculate_earth_mover(prediction, ground_truth)\n",
    "\n",
    "        print(f'The macroaver_aged mean absolute error of the model is {mae}')\n",
    "        print(f\"The Earth Mover's Distance of the model is {emd}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8bad156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_file, test_file):\n",
    "    assert os.path.isfile(model_file), 'Model file does not exist'\n",
    "    assert os.path.isfile(test_file), 'Data file does not exist'\n",
    "\n",
    "    # load training_file\n",
    "    test_file = open(test_file, \"rb\")\n",
    "    data = pickle.load(test_file)\n",
    "    test_file.close()\n",
    "    \n",
    "    # prepare data, remove stopwords, replace unfrequent words and create a token\n",
    "    # to index dictionary\n",
    "    token_to_idx_test = prepare_data(data)\n",
    "    \n",
    "    X = data[\"filtered_text\"].tolist()\n",
    "    Y = data[\"sentiment\"].tolist()\n",
    "    vader_sentiment = data[\"vader_sentiment\"].tolist()\n",
    "    \n",
    "    # load the saved files, only need to run when already saved the files\n",
    "    tokenizer_file = open(\"drive_pkl/tokenizer.pkl\", \"rb\")\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "    tokenizer_file.close()\n",
    "    token_to_idx = tokenizer['token_to_idx']\n",
    "    \n",
    "    # initialize the RNN model\n",
    "    sentiment_to_idx = {\"Highly Negative\":1, \"Negative\":2, \"Neutral\":3, \"Positive\":4, \"Highly Positive\":5}\n",
    "    model = RNNTagger(token_to_idx, sentiment_to_idx) \n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()\n",
    "    \n",
    "    # dataset generator\n",
    "    test_set = Dataset(X, Y, token_to_idx, vader_sentiment)\n",
    "    test_generator = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    prediction = None\n",
    "    ground_truth = None\n",
    "    state_h, state_c = model.init_state()\n",
    "    for sequence, targets in tqdm(test_generator):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        if sequence.shape[0] != 10:\n",
    "             state_h, state_c = (torch.zeros(NUM_LAYERS, sequence.shape[0], HIDDEN_DIM),\n",
    "                torch.zeros(NUM_LAYERS, sequence.shape[0], HIDDEN_DIM))\n",
    "\n",
    "        with torch.no_grad():   \n",
    "            \n",
    "            tag_space, (state_h, state_c) = model(sequence, (state_h, state_c))\n",
    "            \n",
    "            # Store history to calculate accuracy\n",
    "            for i in range(sequence.shape[0]):\n",
    "                if prediction is None:\n",
    "                    prediction = torch.argmax(tag_space[i,:]).cpu().numpy()\n",
    "                else:\n",
    "                    prediction = np.append(prediction, torch.argmax(tag_space[i,:]).cpu().numpy())\n",
    "                if ground_truth is None:\n",
    "                    ground_truth = targets[i].cpu().numpy()\n",
    "                else:\n",
    "                    ground_truth = np.append(ground_truth, targets[i].cpu().numpy())\n",
    "                                \n",
    "    mae = calculate_MAE_macro(prediction, ground_truth)\n",
    "    emd = calculate_earth_mover(prediction, ground_truth)\n",
    "\n",
    "    print(f'The macroaver_aged mean absolute error of the model is {mae}')\n",
    "    print(f\"The Earth Mover's Distance of the model is {emd}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ad452",
   "metadata": {},
   "source": [
    "### Run the training ans testing command for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_drive_data(\"Twitter-sentiment-self-drive-DFE.csv\")\n",
    "split_data(data)\n",
    "model = train(\"drive_pkl/train_drive.pkl\")\n",
    "torch.save(model.state_dict(), \"RNN_drive_model_no_exclamation_no_vader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"RNN_drive_model_no_exclamation_no_vader\", \"drive_pkl/test_drive.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c776ac9",
   "metadata": {},
   "source": [
    "### Run Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e708226",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ml = read_drive_data(\"Twitter-sentiment-self-drive-DFE.csv\")\n",
    "split_data(data_ml)\n",
    "\n",
    "train_file = open(\"drive_pkl/train_drive.pkl\", \"rb\")\n",
    "data_ml = pickle.load(train_file)\n",
    "train_file.close()\n",
    "\n",
    "train_tfidf_matrix = extract_feature(data_ml)\n",
    "data_tfidf = pd.DataFrame(train_tfidf_matrix.todense())\n",
    "data_vader = pd.concat([data_tfidf, data_ml[\"vader_sentiment\"].reset_index(drop=True)], axis = 1)\n",
    "x_train_tfidf, x_valid_tfidf, y_train, y_valid = train_test_split(data_tfidf,data_ml['sentiment'],test_size=0.1,random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdcce6b",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d37694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, multi_class=\"multinomial\").fit(x_train_tfidf, y_train)\n",
    "clf_pred = clf.predict(x_valid_tfidf)\n",
    "print(calculate_MAE_macro(clf_pred, y_valid))\n",
    "print(calculate_earth_mover(clf_pred, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8858db0",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "model = LGBMClassifier(boosting_type='gbdt',\n",
    "    objective='multiclass',\n",
    "    learning_rate=0.05,\n",
    "    num_iteration=110,\n",
    "    num_leaves=24,\n",
    "    max_depth=15,\n",
    "    num_class=5)\n",
    "\n",
    "model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "predicted_LGBM = model.predict(x_valid_tfidf)\n",
    "\n",
    "print(calculate_MAE_macro(predicted_LGBM, y_valid))\n",
    "print(calculate_earth_mover(predicted_LGBM, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fa9dd",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dct = DecisionTreeClassifier(\n",
    "    criterion='gini', \n",
    "    random_state=1,\n",
    "    splitter='random',\n",
    "    max_leaf_nodes=30,\n",
    "\n",
    "    max_depth=15,\n",
    "    min_samples_split=5\n",
    ")\n",
    "dct.fit(x_train_tfidf, y_train)\n",
    "dct_pred = dct.predict(x_valid_tfidf)\n",
    "print(calculate_MAE_macro(dct_pred,y_valid))\n",
    "print(calculate_earth_mover(dct_pred,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd14960",
   "metadata": {},
   "source": [
    "### Decision tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f598869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "def round_float(x):\n",
    "    x = int(round(x))\n",
    "    return x\n",
    "\n",
    "dctr = DecisionTreeRegressor(random_state = 1)\n",
    "dctr.fit(x_train_tfidf, y_train.astype(float))\n",
    "dctr_pred = dctr.predict(x_valid_tfidf)\n",
    "dctr_pred = map(round_float, dctr_pred)\n",
    "dctr_pred = list(dctr_pred)\n",
    "\n",
    "print(calculate_MAE_macro(dctr_pred,y_valid))\n",
    "print(calculate_earth_mover(dctr_pred,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9effae",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "def round_float(x):\n",
    "    x = int(round(x))\n",
    "    return x\n",
    "svr = make_pipeline(StandardScaler(), SVR(C=4.0, epsilon=0.2))\n",
    "svr.fit(x_train_tfidf, y_train.astype(float))\n",
    "svr_pred = svr.predict(x_valid_tfidf)\n",
    "svr_pred = map(round_float,svr_pred)\n",
    "svr_pred = list(svr_pred)\n",
    "\n",
    "print(calculate_MAE_macro(svr_pred,y_valid))\n",
    "print(calculate_earth_mover(svr_pred,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76934ac",
   "metadata": {},
   "source": [
    "### LightGBM regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bba968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "def round_float(x):\n",
    "    x = int(round(x))\n",
    "    return x\n",
    "\n",
    "lgbmr = LGBMRegressor(boosting_type='gbdt',\n",
    "    objective='regression',\n",
    "    learning_rate=0.05,\n",
    "    num_iteration=100,\n",
    "    num_leaves=32,\n",
    "    max_depth=8)\n",
    "\n",
    "lgbmr.fit(x_train_tfidf, y_train)\n",
    "lgbmr_pred = lgbmr.predict(x_valid_tfidf)\n",
    "lgbmr_pred = map(round_float,lgbmr_pred)\n",
    "lgbmr_pred = list(lgbmr_pred)\n",
    "\n",
    "print(calculate_MAE_macro(lgbmr_pred, y_valid))\n",
    "print(calculate_earth_mover(lgbmr_pred, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
