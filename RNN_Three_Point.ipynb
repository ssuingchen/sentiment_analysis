{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle, argparse, os, sys\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "\n",
    "import torch.functional as F\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23780787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 16\n",
    "NUM_LAYERS = 1\n",
    "MAX_SEQ_LENGTH = 20\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCH = 3\n",
    "BATCH_SIZE = 10\n",
    "WEIGHT_DECAY = 0\n",
    "GLOVE_PATH = \"glove.6B 2\"\n",
    "DROPOUT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e5944",
   "metadata": {},
   "source": [
    "### Manually checked the quality of the lables and decided to remove rows that have confidence less than 0.4. The removed ones accounted for 1.6 percent of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636f8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_airline_data(data_path = None):\n",
    "    '''\n",
    "        read csv data and filter rows with confidence level of the sentiment label lower than 0.4\n",
    "        \n",
    "        parms: data path str\n",
    "        \n",
    "        return: pandas dataframe\n",
    "    '''\n",
    "    #vader sentiment scorer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    data = pd.read_csv(data_path)\n",
    "    data = data[[\"airline_sentiment\", \"airline_sentiment_confidence\", \"airline\", \"text\"]]\n",
    "    data = data[data[\"airline_sentiment_confidence\"] > .4]\n",
    "    data[\"vader_sentiment\"] = data[\"text\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "    data = data.reset_index(drop = True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b993b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    '''\n",
    "        Split data into two parts\n",
    "        \n",
    "        params: pandas dataframe\n",
    "        return: two pandas dataframe and save as pickle files\n",
    "    '''\n",
    "    # split data into training_validation(0.9) and testing sets(0.1)\n",
    "    train_valid, test = train_test_split(data, test_size=0.1, random_state=42, shuffle=True, \\\n",
    "                                         stratify=data[\"airline_sentiment\"])\n",
    "\n",
    "    train_pkl = open(\"airline_pkl/train.pkl\", \"wb\")\n",
    "    pickle.dump(train_valid, train_pkl)\n",
    "    train_pkl.close()\n",
    "    \n",
    "    test_pkl = open(\"airline_pkl/test.pkl\", \"wb\")\n",
    "    pickle.dump(test, test_pkl)\n",
    "    test_pkl.close()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05409903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    '''\n",
    "        create token list for the tweets and create a vocabulary dictionary \n",
    "        to replace unfrequent words with UNKA\n",
    "        \n",
    "        Also create a token_to_idx dictionary include special token <PAD>\n",
    "        \n",
    "        Besides, convert sentiment label into integers and create one hot vectors\n",
    "\n",
    "        param: pandas dataframe\n",
    "        return: token_to_idx, dict\n",
    "                #one hot vector embedding of labels, tensor\n",
    "    '''\n",
    "    \n",
    "    # remove stopwords\n",
    "    # tokenize tweets\n",
    "    # create vocab list for checking frequency\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text_list = []\n",
    "    vocab_list = []\n",
    "    for tweet in tqdm(data[\"text\"]):\n",
    "        tweet = re.sub(\"\\@[A-Za-z]+\", \"\", tweet)\n",
    "        tweet = re.sub(\"http\\S+\", \"\", tweet)\n",
    "        tweet = re.sub(\"[.!,@#$%^&*]\", \"\", tweet)\n",
    "        tokens = word_tokenize(tweet)\n",
    "        filtered_text = [w for w in tokens if not w in stop_words] \n",
    "        filtered_text_list.append(filtered_text)\n",
    "        vocab_list += filtered_text\n",
    "    data[\"filtered_text\"] = filtered_text_list\n",
    "    \n",
    "    # replace unfrequent words with \"UNKA\"\n",
    "    # create token_to_idx dict\n",
    "    vocab_dict = Counter(vocab_list)\n",
    "    token_to_replace = {k: v for k, v in vocab_dict.items() if v < 3}\n",
    "    token_to_idx = {\"<PAD>\":0}\n",
    "    token_list = []\n",
    "    \n",
    "    for tweet in tqdm(data[\"filtered_text\"]):\n",
    "        for token_idx in range(len(tweet)):\n",
    "            if tweet[token_idx] in token_to_replace:\n",
    "                tweet[token_idx] = \"UNKA\"\n",
    "        token_list += tweet\n",
    "    for token in Counter(token_list).keys():\n",
    "        if token not in token_to_idx:\n",
    "            token_to_idx[token] = len(token_to_idx)\n",
    "            \n",
    "    # create index for sentiment labels\n",
    "    sentiment_to_idx = {\"neutral\":0, \"negative\":1, \"positive\":2}\n",
    "    data[\"label_idx\"] = data[\"airline_sentiment\"].apply(lambda x: sentiment_to_idx[x])\n",
    "\n",
    "    return token_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a50676c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecnode_sequence(tweet, token_to_idx):\n",
    "    '''\n",
    "        convert tweet into sequence of index using token_to_idx dict\n",
    "\n",
    "        params: str and token to index dictionary\n",
    "        return: eccoded sequence list in tensor\n",
    "    '''\n",
    "    encoded_seq_list = []\n",
    "    \n",
    "    for token in tweet:\n",
    "        try:\n",
    "            encoded_seq_list.append(token_to_idx[token])\n",
    "        except KeyError:\n",
    "            encoded_seq_list.append(token_to_idx[\"UNKA\"])\n",
    "            \n",
    "    return torch.tensor(encoded_seq_list, dtype=torch.long)\n",
    "\n",
    "def pad_sequence(encoded_sequence, max_seq_length, vader_sentiment):\n",
    "    # with vader sentiment feature\n",
    "    '''\n",
    "        truncate or pad the sequence with 0 if the sequence is shorter \n",
    "        than the number defined for training:max_seq_length\n",
    "        \n",
    "        parmas: tensor of encoded sequence\n",
    "        return: tensor of padded sequence\n",
    "        \n",
    "    '''\n",
    "    padded_sentence = torch.zeros(max_seq_length, dtype = torch.long)\n",
    "    value_to_pad = min(len(encoded_sequence), max_seq_length)\n",
    "    padded_sentence[:value_to_pad] = encoded_sequence[:value_to_pad]\n",
    "    padded_sentence[-1] = vader_sentiment\n",
    "    return padded_sentence\n",
    "\n",
    "\n",
    "# def pad_sequence(encoded_sequence, max_seq_length):\n",
    "#     '''\n",
    "#         truncate or pad the sequence with 0 if the sequence is shorter \n",
    "#         than the number defined for training:max_seq_length\n",
    "        \n",
    "#         parmas: tensor of encoded sequence\n",
    "#         return: tensor of padded sequence\n",
    "        \n",
    "#     '''\n",
    "#     padded_sentence = torch.zeros(max_seq_length, dtype = torch.long)\n",
    "#     value_to_pad = min(len(encoded_sequence), max_seq_length)\n",
    "#     padded_sentence[:value_to_pad] = encoded_sequence[:value_to_pad]\n",
    "#     return padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16bb2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(glove_path):\n",
    "    '''\n",
    "        open the glove pre-trained embeddings and process it\n",
    "        save the word embedding vectors to pytorch tensor and \n",
    "        the words and word_to_idx dictionary to pickle files\n",
    "\n",
    "        params: glove path\n",
    "        return: word embedding tensor\n",
    "    '''\n",
    "    words = []\n",
    "    idx = 0\n",
    "    word2idx = {}\n",
    "    vectors = []\n",
    "\n",
    "    stop = 0\n",
    "    with open(f'{glove_path}/glove.6B.50d.txt', 'rb') as f:\n",
    "        for l in f:\n",
    "            line = l.decode().split()\n",
    "            word = line[0]\n",
    "            words.append(word)\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "            vect = np.array(line[1:]).astype(np.float)\n",
    "            vect = torch.tensor(vect, dtype=torch.double)\n",
    "            vect = torch.reshape(vect, [1, 50])\n",
    "            vectors.append(vect)\n",
    "\n",
    "    vectors = torch.cat(vectors)\n",
    "    pickle.dump(words, open(f'{glove_path}/6B.50_words.pkl', 'wb'))\n",
    "    pickle.dump(word2idx, open(f'{glove_path}/6B.50_idx.pkl', 'wb'))\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4d0edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_glove(vectors, glove_path, token_to_idx):\n",
    "    '''\n",
    "        open the words and word_to_index pickle files\n",
    "        and map every token in the training data with a vector\n",
    "\n",
    "    '''\n",
    "    # load glove embeddings\n",
    "    words = pickle.load(open(f'{glove_path}/6B.50_words.pkl', 'rb'))\n",
    "    word2idx = pickle.load(open(f'{glove_path}/6B.50_idx.pkl', 'rb'))\n",
    "    glove = {w: vectors[word2idx[w]] for w in words}\n",
    "\n",
    "    # load training data\n",
    "    embedding_weights = np.zeros((len(token_to_idx), EMBEDDING_DIM))\n",
    "\n",
    "    for token, index in token_to_idx.items():\n",
    "        if token == \"UNKA\":\n",
    "            embedding_weights[index, :] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "        elif index == 0: #padding\n",
    "            embedding_weights[index, :] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "        else:\n",
    "            try:\n",
    "                embedding_weights[index, :] = glove[token]\n",
    "            except KeyError:\n",
    "                embedding_weights[index, :] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "\n",
    "    return torch.tensor(embedding_weights, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f4f1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    def __init__(self, token_to_idx, sentiment_to_idx):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        self.embedding_dim = EMBEDDING_DIM\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.vocab_size = len(token_to_idx)\n",
    "        self.tagset_size = len(sentiment_to_idx)\n",
    "        self.bidirectional = False\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size, \n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx = 0\n",
    "            )\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim, \n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first = True,\n",
    "            bidirectional=self.bidirectional)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        if self.bidirectional:\n",
    "            self.hidden2tag = nn.Linear(2 * self.hidden_dim, self.tagset_size)\n",
    "        else:\n",
    "            self.hidden2tag = nn.Linear(self.hidden_dim, self.tagset_size)\n",
    "            \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sentence, prev_state):\n",
    "        sentence = sentence.long()\n",
    "        embeds = self.word_embedding(sentence)\n",
    "        lstm_out, state = self.lstm(embeds, prev_state)\n",
    "        # add a fully connected layer to convert the high level info into our goal\n",
    "        # only use the last output of the lstm layer for many to one\n",
    "        lstm_out = self.hidden2tag(lstm_out[:, -1, :])\n",
    "        return lstm_out, state\n",
    "\n",
    "    def load_embedding(self, embedding_weights):\n",
    "        # using pre-trained embedding\n",
    "        self.word_embedding.load_state_dict({'weight': embedding_weights})\n",
    "\n",
    "    def init_state(self):\n",
    "        if self.bidirectional:\n",
    "            return (torch.zeros(2 * self.num_layers, BATCH_SIZE, self.hidden_dim),\n",
    "                    torch.zeros(2 * self.num_layers, BATCH_SIZE, self.hidden_dim))\n",
    "        else:\n",
    "            return (torch.zeros(self.num_layers, BATCH_SIZE, self.hidden_dim),\n",
    "                    torch.zeros(self.num_layers, BATCH_SIZE, self.hidden_dim))\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seq_list, label_list, token_to_idx, vader_sentiment):\n",
    "        self.seq_list = seq_list\n",
    "        self.label_list = label_list\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.vader_sentiment = vader_sentiment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        # with vader sentiment feature\n",
    "        seq = self.seq_list[index]\n",
    "        label_list = self.label_list[index]\n",
    "        vader_score = self.vader_sentiment[index]\n",
    "\n",
    "        sentence_input = ecnode_sequence(seq, self.token_to_idx)\n",
    "        padded_sentence = pad_sequence(sentence_input, MAX_SEQ_LENGTH, vader_score)\n",
    "        return padded_sentence, label_list\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # Select sample\n",
    "#         seq = self.seq_list[index]\n",
    "#         label_list = self.label_list[index]\n",
    "\n",
    "#         sentence_input = ecnode_sequence(seq, self.token_to_idx)\n",
    "#         padded_sentence = pad_sequence(sentence_input, MAX_SEQ_LENGTH)\n",
    "# #         padded_targets = pad_sequence(targets, MAX_SEQ_LENGTH)\n",
    "# #         seq_len = len(seq)\n",
    "\n",
    "#         return padded_sentence, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82b4f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(prediction, ground_truth):\n",
    "    '''\n",
    "        calculate the f1 score for positive and\n",
    "        negative labels and the overall f1 score\n",
    "        \n",
    "        params: arrays\n",
    "        return: three f1 scores\n",
    "    '''\n",
    "    accuracy_table = torch.zeros(3,3)\n",
    "\n",
    "    for pred, truth in zip(prediction, ground_truth):\n",
    "        if (pred, truth) == (2, 2):\n",
    "            accuracy_table[0][0] += 1\n",
    "        elif (pred, truth) == (2, 0):\n",
    "            accuracy_table[0][1] += 1\n",
    "        elif (pred, truth) == (2, 1):\n",
    "            accuracy_table[0][2] += 1\n",
    "        elif (pred, truth) == (0, 2):\n",
    "            accuracy_table[1][0] += 1\n",
    "        elif (pred, truth) == (0, 0):\n",
    "            accuracy_table[1][1] += 1\n",
    "        elif (pred, truth) == (0, 1):\n",
    "            accuracy_table[1][2] += 1\n",
    "        elif (pred, truth) == (1, 2):\n",
    "            accuracy_table[2][0] += 1\n",
    "        elif (pred, truth) == (1, 0):\n",
    "            accuracy_table[2][1] += 1\n",
    "        elif (pred, truth) == (1, 1):\n",
    "            accuracy_table[2][2] += 1\n",
    "\n",
    "\n",
    "    presition_pos = accuracy_table[0][0] / (accuracy_table[0][0] +  accuracy_table[0][1] + accuracy_table[0][2])\n",
    "    recall_pos = accuracy_table[0][0] / (accuracy_table[0][0] +  accuracy_table[1][0] + accuracy_table[2][0])\n",
    "    presition_neg = accuracy_table[2][2] / (accuracy_table[2][0] +  accuracy_table[2][1] + accuracy_table[2][2])\n",
    "    recall_neg = accuracy_table[2][2] / (accuracy_table[0][2] +  accuracy_table[1][2] + accuracy_table[2][2])\n",
    "\n",
    "    f1_score_pos = (2 * presition_pos * recall_pos) / (presition_pos + recall_pos)\n",
    "    f1_score_neg = (2 * presition_neg * recall_neg) / (presition_neg + recall_neg)\n",
    "    f1_score = (f1_score_pos + f1_score_neg) / 2\n",
    "\n",
    "    return f1_score_pos, f1_score_neg, f1_score, accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "239bdeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_file):\n",
    "    # load training_file\n",
    "    train_file = open(training_file, \"rb\")\n",
    "    data = pickle.load(train_file)\n",
    "    train_file.close()\n",
    "    \n",
    "    # prepare data, remove stopwords, replace unfrequent words and create a token\n",
    "    # to index dictionary\n",
    "    token_to_idx = prepare_data(data)\n",
    "    \n",
    "    X = data[\"filtered_text\"].tolist()\n",
    "    Y = data[\"label_idx\"].tolist()\n",
    "    vader_sentiment = data[\"vader_sentiment\"].tolist()\n",
    "    \n",
    "#     # create a tokenizer pickle file for cache, only need to run when first executing\n",
    "    tokenizer = {'token_to_idx': token_to_idx}\n",
    "    tokenizer_file = open(\"airline_pkl/tokenizer.pkl\", \"wb\")\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n",
    "    tokenizer_file.close()\n",
    "    \n",
    "    # create glove vectors\n",
    "    vectors = load_glove(GLOVE_PATH)\n",
    "    # create word embedding weight matrix for training\n",
    "    embedding_weights = word_to_glove(vectors, GLOVE_PATH, token_to_idx)\n",
    "    torch.save(embedding_weights, 'airline_pkl/embedding_weights.pt')\n",
    "        \n",
    "    # load the saved files, only need to run when already saved the files\n",
    "    tokenizer_file = open(\"airline_pkl/tokenizer.pkl\", \"rb\")\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "    tokenizer_file.close()\n",
    "    token_to_idx = tokenizer['token_to_idx']\n",
    "    embedding_weights = torch.load('airline_pkl/embedding_weights.pt')\n",
    "\n",
    "    # initialize the RNN model\n",
    "    sentiment_to_idx = {\"neutral\":0, \"negative\":1, \"positive\":2}\n",
    "    model = RNNTagger(token_to_idx, sentiment_to_idx) \n",
    "    model.load_embedding(embedding_weights)\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # dataset generator\n",
    "    training_set = Dataset(X, Y, token_to_idx, vader_sentiment)\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    print('Start training...')\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        print('Epoch %d:' % (epoch+1))\n",
    "        state_h, state_c = model.init_state()\n",
    "        \n",
    "        prediction = None\n",
    "        ground_truth = None\n",
    "        losses = []\n",
    "        for sequence, targets in tqdm(training_generator):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            \n",
    "            if sequence.shape[0] != 10:\n",
    "                 state_h, state_c = (torch.zeros(NUM_LAYERS, sequence.shape[0], HIDDEN_DIM),\n",
    "                    torch.zeros(NUM_LAYERS, sequence.shape[0], HIDDEN_DIM))\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "             \n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_space, (state_h, state_c) = model(sequence, (state_h, state_c))\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            targets = targets.long()\n",
    "            loss = loss_function(tag_space, targets)\n",
    "\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            # Store history to calculate accuracy\n",
    "            for i in range(sequence.shape[0]):\n",
    "                if prediction is None:\n",
    "                    prediction = torch.argmax(tag_space[i,:]).cpu().numpy()\n",
    "                else:\n",
    "                    prediction = np.append(prediction, torch.argmax(tag_space[i,:]).cpu().numpy())\n",
    "                if ground_truth is None:\n",
    "                    ground_truth = targets[i].cpu().numpy()\n",
    "                else:\n",
    "                    ground_truth = np.append(ground_truth, targets[i].cpu().numpy())\n",
    "                                \n",
    "        losses = torch.tensor(losses)\n",
    "        train_perplexity = math.exp(torch.mean(losses))\n",
    "        print(prediction)\n",
    "        print(ground_truth)\n",
    "        f1_score_pos, f1_score_neg, f1_score, accuracy_table = calculate_f1(prediction, ground_truth)\n",
    "        print(accuracy_table)\n",
    "        print(f'The f1 score of the pos labels is {100*f1_score_pos:6.2f}%')\n",
    "        print(f'The f1 score of the neg labels is {100*f1_score_neg:6.2f}%')\n",
    "        print(f'The f1 score of the model is {100*f1_score:6.2f}%')\n",
    "        print(f'The perplexity of the model is {train_perplexity}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5528082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_file, test_file):\n",
    "    assert os.path.isfile(model_file), 'Model file does not exist'\n",
    "    assert os.path.isfile(test_file), 'Data file does not exist'\n",
    "\n",
    "    # load training_file\n",
    "    test_file = open(test_file, \"rb\")\n",
    "    data = pickle.load(test_file)\n",
    "    test_file.close()\n",
    "    \n",
    "    # prepare data, remove stopwords, replace unfrequent words and create a token\n",
    "    # to index dictionary\n",
    "    token_to_idx_test = prepare_data(data)\n",
    "    \n",
    "    X = data[\"filtered_text\"].tolist()\n",
    "    Y = data[\"label_idx\"].tolist()\n",
    "    vader_sentiment = data[\"vader_sentiment\"].tolist()\n",
    "    \n",
    "    # load the saved files, only need to run when already saved the files\n",
    "    tokenizer_file = open(\"airline_pkl/tokenizer.pkl\", \"rb\")\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "    tokenizer_file.close()\n",
    "    token_to_idx = tokenizer['token_to_idx']\n",
    "    \n",
    "    # initialize the RNN model\n",
    "    sentiment_to_idx = {\"neutral\":0, \"negative\":1, \"positive\":2}\n",
    "    model = RNNTagger(token_to_idx, sentiment_to_idx) \n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()\n",
    "    \n",
    "    # dataset generator\n",
    "    test_set = Dataset(X, Y, token_to_idx, vader_sentiment)\n",
    "    test_generator = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    prediction = None\n",
    "    ground_truth = None\n",
    "    state_h, state_c = model.init_state()\n",
    "    for sequence, targets in tqdm(test_generator):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        if sequence.shape[0] != 10:\n",
    "             state_h, state_c = (torch.zeros(NUM_LAYERS, sequence.shape[0], HIDDEN_DIM),\n",
    "                torch.zeros(NUM_LAYERS, sequence.shape[0], HIDDEN_DIM))\n",
    "\n",
    "        with torch.no_grad():   \n",
    "            \n",
    "            tag_space, (state_h, state_c) = model(sequence, (state_h, state_c))\n",
    "            \n",
    "            # Store history to calculate accuracy\n",
    "            for i in range(sequence.shape[0]):\n",
    "                if prediction is None:\n",
    "                    prediction = torch.argmax(tag_space[i,:]).cpu().numpy()\n",
    "                else:\n",
    "                    prediction = np.append(prediction, torch.argmax(tag_space[i,:]).cpu().numpy())\n",
    "                if ground_truth is None:\n",
    "                    ground_truth = targets[i].cpu().numpy()\n",
    "                else:\n",
    "                    ground_truth = np.append(ground_truth, targets[i].cpu().numpy())\n",
    "                                \n",
    "    print(prediction)\n",
    "    print(ground_truth)\n",
    "    f1_score_pos, f1_score_neg, f1_score, accuracy_table = calculate_f1(prediction, ground_truth)\n",
    "    print(accuracy_table)\n",
    "    print(f'The f1 score of the pos labels is {100*f1_score_pos:6.2f}%')\n",
    "    print(f'The f1 score of the neg labels is {100*f1_score_neg:6.2f}%')\n",
    "    print(f'The f1 score of the model is {100*f1_score:6.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc2d22",
   "metadata": {},
   "source": [
    "### Run the training and testing command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_airline_data(data_path = \"Tweets_airlines.csv\")\n",
    "split_data(data)\n",
    "model = train(\"airline_pkl/train.pkl\")\n",
    "torch.save(model.state_dict(), \"RNN_airline_model_no_exclamation_with_vader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458180ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(\"RNN_airline_model_no_exclamation_with_vader\", \"airline_pkl/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e20d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
